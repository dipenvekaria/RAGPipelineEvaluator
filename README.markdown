# RAG Pipeline Evaluator

## Overview

The **RAG Pipeline Evaluator** is a web application designed to compare different Retrieval-Augmented Generation (RAG) pipeline configurations for processing and querying PDF documents. The app allows users to upload PDF files, process them with various RAG configurations (embedding models, chunk sizes, similarity thresholds, and retrieval limits), and compare the quality of responses to user queries across two pipeline configurations. Users can vote on which pipeline produces better results, and the app stores these votes for analysis. The application is built to run on Hugging Face Spaces, leveraging a modern tech stack for efficient document processing and user interaction.

### Goal and Functionality

The primary goal of the RAG Pipeline Evaluator is to enable users to evaluate and optimize RAG pipelines by comparing how different configurations affect the quality of answers generated from PDF documents. Key functionalities include:

- **PDF Upload and Processing**: Users can upload PDF files, which are processed into text chunks using different RAG pipeline configurations (e.g., OpenAI embedding models, chunk sizes of 300, 500, or 800, similarity thresholds of 0.3 or 0.5, and retrieval limits of 3, 5, or 7).
- **Querying**: Users can submit questions related to the uploaded PDFs, and the app retrieves relevant text chunks using the selected pipeline configurations to generate answers via an LLM (GPT-4o-mini).
- **Pipeline Comparison**: The app displays answers from two user-defined pipeline configurations side by side, allowing direct comparison of their performance.
- **Voting**: Users can vote for the pipeline that provides the better answer, and these votes are stored for future analysis.
- **Document Management**: Users can delete uploaded PDFs, removing them from both the local storage and the vector database.

The app is pre-loaded with popular stories like *Goldilocks* and *The Gingerbread Man* for testing. Example queries include “What did Goldilocks do?” or “What happened to the Gingerbread Man?”

## How to Use the App

### Prerequisites

To use the app on Hugging Face Spaces, ensure the following:
- A Hugging Face account to access the Space.
- API keys for OpenAI (for embeddings and LLM) and Qdrant (for vector storage), set as Secrets in the Hugging Face Space settings.

### Setup (For Deployment on Hugging Face Spaces)

1. **Create a Space**:
   - Go to [Hugging Face Spaces](https://huggingface.co/spaces) and create a new Space.
   - Select "Gradio" as the SDK and choose the free plan.
   - Name your Space (e.g., `rag-pipeline-evaluator`).

2. **Configure Secrets**:
   - In the Space’s Settings, navigate to the "Secrets" section.
   - Add the following environment variables:
     - `OPENAI_API_KEY`: Your OpenAI API key.
     - `QDRANT_URL`: The URL of your Qdrant cluster.
     - `QDRANT_API_KEY`: Your Qdrant API key.

3. **Upload Files**:
   - Upload the following files to the root of your Space:
     - `app.py`: The main application code.
     - `requirements.txt`: The dependency list.
   - You can upload via the Hugging Face web interface or by linking a Git repository.

4. **Deploy**:
   - Hugging Face will automatically build and deploy the Space. Monitor the build logs for any issues with dependency installation.

### Using the App

Once the Space is running, follow these steps to use the RAG Pipeline Evaluator:

1. **Upload a PDF**:
   - Open the "File Upload" accordion on the right side of the interface.
   - Drag and drop a PDF file (e.g., a document containing *Goldilocks* or *The Gingerbread Man*) or click to select one.
   - Click the "Upload PDF" button. The app will process the PDF with all possible pipeline configurations and display a confirmation message (e.g., "Processed: filename.pdf").

2. **Configure Pipelines**:
   - In the main interface, configure two pipelines (A and B) by selecting:
     - **Model**: Choose between `text-embedding-3-small` or `text-embedding-3-large`.
     - **Chunk Size**: Select 300, 500, or 800.
     - **Similarity Threshold**: Choose 0.3 or 0.5.
     - **Retrieval Limit**: Select 3, 5, or 7.
   - Pipeline A and B should have different configurations for meaningful comparison.

3. **Ask a Question**:
   - Enter a question in the text box at the top (e.g., “What did Goldilocks do?”).
   - Click the "Compare Pipelines" button.
   - The app will display the answers generated by Pipeline A and Pipeline B in separate text boxes below.

4. **Vote on Results**:
   - Review the answers from both pipelines.
   - Click "Vote for Pipeline A" or "Vote for Pipeline B" to indicate which pipeline provided the better answer.
   - A confirmation message (e.g., "Vote recorded for Pipeline A") will appear.

5. **Manage Documents**:
   - Open the "Manage Documents" accordion.
   - Select a previously uploaded file from the dropdown.
   - Click "Delete Selected File" to remove the file from the app and the Qdrant database. A confirmation or error message will be displayed.

### Example Queries
- “What did Goldilocks do in the bears’ house?”
- “Who chased the Gingerbread Man?”
- “What happened to the Gingerbread Man at the end?”

## Technology Stack

The RAG Pipeline Evaluator is built with a modern Python-based tech stack, optimized for Hugging Face Spaces. Below is the complete list of technologies used:

- **Python 3.10+**: The programming language for the application logic.
- **Gradio (4.44.0)**: A Python library for creating the web-based user interface, hosted on Hugging Face Spaces.
- **PyMuPDF (1.24.7)**: A library for extracting text from PDF files.
- **Qdrant Client (1.12.0)**: A client for interacting with the Qdrant vector database, used to store and query text embeddings.
- **LangChain-OpenAI (0.2.1)**: A library for integrating OpenAI’s embedding models (`text-embedding-3-small`, `text-embedding-3-large`) and LLM (`gpt-4o-mini`) for text processing and answer generation.
- **Scikit-Learn (1.5.1)**: Used for computing cosine similarity during semantic chunking.
- **NumPy (1.26.4)**: A dependency for numerical computations, required by Scikit-Learn.
- **Python-dotenv (1.0.1)**: Used for local development to load environment variables (not used in Hugging Face Spaces).
- **OpenAI API**: Provides embedding models and the GPT-4o-mini LLM for generating answers.
- **Qdrant**: A vector database for storing and retrieving text embeddings with cosine similarity search.
- **Hugging Face Spaces**: The hosting platform, providing a free-tier containerized environment for running the Gradio app.

### Dependencies (requirements.txt)
```
PyMuPDF==1.24.7
gradio==4.44.0
qdrant-client==1.12.0
langchain-openai==0.2.1
scikit-learn==1.5.1
numpy==1.26.4
python-dotenv==1.0.1
```

## Notes for Developers

- **Hugging Face Spaces Limitations**: The free plan has limited CPU and memory. Processing large PDFs or using many pipeline configurations may hit resource limits. Consider testing with smaller documents initially.
- **Environment Variables**: Ensure `OPENAI_API_KEY`, `QDRANT_URL`, and `QDRANT_API_KEY` are set in the Space’s Secrets to avoid runtime errors.
- **Local Testing**: To test locally, install the dependencies from `requirements.txt` and set the required environment variables using a `.env` file or command-line exports.
- **Extending the App**: To add new embedding models or pipeline parameters, modify the `embedding_models`, `chunk_sizes`, `similarity_thresholds`, and `retrieval_limits` in the `RAGEvaluator` class.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details (if applicable).

